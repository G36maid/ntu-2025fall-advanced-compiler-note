# Data Flow Analysis & Global Optimization

## Introduction to Optimization

### Why We Need Optimization in Compilers
*   Simplify the task of intermediate representation (IR) generation in the front-end.
*   Improve the quality of the code generated by the back-end.
*   Bridge the gap between processor speed and application performance.

### Different Objectives for Designing the Optimizer
*   Runtime speed
*   Code size
*   Register use
*   Memory use
*   Energy Consumption
*   Number of Page Faults

### Two Considerations for Optimization
1.  **Safety**: After applying the IR transformation, it preserves the program’s meaning.
2.  **Profitability**: After applying the IR transformation, it improves the program’s performance.

## Different Granularities of Optimization

*   **Local Methods**: Operate over a single basic block. Operate on the three-address code.
*   **Regional Methods**: Operate over scopes larger than a single basic block but smaller than a full procedure. Operate on the control-flow graph (CFG). For example, the compiler might consider the entire loop as a single region.
*   **Global Methods**: Operate over an entire procedure. Operate on the control-flow graph. Decisions that are locally optimal may have bad consequences in some larger context.
*   **Whole-program Methods**: Operate over the entire program. Operate on the program’s call graph.

## Global Optimization

Optimizations that examine an entire procedure have opportunities for improvement that are not available at smaller scopes. Because the scope includes cyclic paths and backward branches, global optimization needs global analysis.

Global optimization consists of two phases:
1.  Analysis phase
2.  Transformation phase

## What is Data-flow Analysis?

*   **Local Analysis**: Analyze effect of each instruction. Compose effects of instructions to derive information from beginning of basic block to each instruction.
*   **Data-flow Analysis**: Analyze effect of each basic block. Compose effects of basic blocks to derive information at basic block boundaries.

Data-flow analysis takes the IR as input and decides whether or not they have a certain property. Rice’s theorem states that all interesting questions about the input/output behavior of programs are undecidable. While it’s impossible to build an analysis that would correctly decide a property for any analyzed program, it is often possible to build analysis tools that give useful answers for most realistic programs.

The approximative properties of the program can help us generate efficient code:
*   Does the program contain dead code? If so, the code size can be reduced.
*   Is the value of some expression inside a loop the same in every iteration? If so, the expression can be moved outside the loop.
*   Does the value of variable x depend on the program input? If not, it could be precomputed at compile time.

## Example of Data-flow Analysis: Liveness

### Definition
A variable `v` is **live** at point `p` if the value of `v` is used along some path in the flow graph starting at `p`. Otherwise, the variable `v` is **dead**.

### Data-flow Equation of Liveness
*   A variable is live on exit from a node if it is live on the entry to any successor node.
*   A variable is live on entry to the node if:
    *   it is used by the node
    *   it is live on exit from the node and the node does not redefine the value of the variable

$$LIVEOUT(n) = \bigcup_{s \in succ(n)} LIVEIN(s)$$
$$LIVEIN(n) = UEVAR(n) \cup (LIVEOUT(n) - VARKILL(n))$$

Where:
*   `LIVEOUT(n)`: The variables that are live at the end of block `n`.
*   `UEVAR(n)`: The variables that are used in block `n` before any redefinition in block `n` (upward-exposed variables).
*   `VARKILL(n)`: The variables that are defined in block `n`.

## Solving the Data-flow Problem

Compiler can use a three-step algorithm to compute `LIVEOUT`:
1.  Build a Control-flow Graph.
2.  Gather initial information: The analyzer computes a `UEVAR` and `VARKILL` set for each block `b` in a simple walk.
3.  Solve the equations to produce `LIVEOUT(b)` for each block `b`: Use the iterative fixed-point algorithm to solve the equations.

### Recap: Basic Blocks & Control-flow Graph

*   **Basic block**: A sequence of 3-address statements.
    *   Only the first statement can be reached from outside the block (no branches into middle of block).
    *   All the statements are executed consecutively if the first one is (no branches out or halts except perhaps at end of block).
    *   We require basic blocks to be maximal, i.e., they cannot be made larger without violating the conditions.
*   **Control Flow Graph (CFG)**:
    *   Nodes: Basic Blocks
    *   Edges: Possible transfer from a Basic Block to another.

### Building a Control-flow Graph from a Linear IR

1.  **Separate the list of instructions into basic blocks.**
    *   Identify the leader of each basic block:
        *   First Instruction
        *   Any target of a jump
        *   Any instruction immediately following a jump
    *   A basic block starts at a leader and ends at the instruction immediately before the next leader (or the last instruction).
2.  **Form the control-flow graph from the basic blocks.**
    *   Store the label names onto the corresponding basic blocks.
    *   For each basic block:
        *   Check whether the last instruction of the block is a terminator.
        *   If it’s a terminator and a jump instruction, add an edge from the block to the destination block.
        *   If it's a terminator and it's a branch instruction, add edges from the block to all of the possible blocks.
        *   If it's not a terminator, if there exists a next-coming block, add an edge from this block to the next-coming block.

### Iterative Fixed-point Algorithm

(Detailed algorithm and example iterations were present in slides, generally involves initializing `LIVEOUT` sets and iteratively updating them until no changes occur.)

### Worklist Algorithm

An alternative to Iterative Fixed-point Algorithm:
1.  Maintain a FIFO queue called the worklist to keep track of nodes whose data-flow equations might not be satisfied at any given step.
2.  Initialize the worklist to contain all nodes.
3.  While the worklist contains some node `n`:
    *   Remove node `n` from the worklist.
    *   Compute the `LIVEOUT(n)` using the data-flow equation.
    *   If `LIVEOUT(n)` changed, push all predecessors of `n` onto the worklist.

### Termination, Correctness, and Efficiency

*   **Termination**:
    *   The finite descending chain property guarantees termination and the termination is independent of the evaluation order.
    *   **Finite descending chain property**:
        *   **Monotonicity**: In the while loop of the algorithm, the `LIVEOUT` sets grow monotonically.
        *   **Finite number of possible values for the underlying sets**: The size of any `LIVEOUT` set is bounded by the number of variables.
        *   In the worst case, one `LIVEOUT` set would grow by one element in each iteration; that behavior would halt after `n * |V|` iterations.
*   **Correctness**:
    *   Iterative fixed-point algorithm is correct if it finds all the variables that satisfy the definition of liveness at the end of each block.
    *   The theory of iterative data-flow analysis assures us that the fixed-point is unique.
    *   The uniqueness of the fixed point guarantees that the fixed-point solution computed by iterative algorithms is identical to the meet-over-all-paths solution called for by the definition.
*   **Efficiency**:
    *   Since the fixed-point solution is unique, the analyzer can choose any order of evaluation in fixed-point algorithm.
    *   The analyzer should choose the one that produces rapid termination.
    *   **Forward data-flow problem**: In the data-flow equation, the information is flowing forward along the arrows in the CFG, e.g., Dominance.
    *   **Backward data-flow problem**: In the data-flow equation, the information is flowing backward along the arrows in the CFG, e.g., Liveness.
    *   For most forward data-flow problems, the reverse postorder produces rapid termination.
    *   For most backward data-flow problems, the reverse postorder on the reverse CFG produces rapid termination.
    *   These orders force the iterative algorithm to evaluate as many predecessors or successors as possible before it evaluates a node `n`.

### Global Optimization with Liveness
We can find potential uses of uninitialized variables by computing information about liveness.
*   **Uninitialized variables**: If a procedure `p` can use the value of some variable `v` before `v` has been assigned a value, we say that `v` is uninitialized at that use.
*   Given a `LIVEOUT` set for the CFG’s entry node `n`, each variable in `LIVEOUT(n)` has a potentially uninitialized use.

### Limitations in Data-flow Analysis
*   **Data-flow analysis is conservative**: Data-flow analysis accounts for the effects along all possible paths through the code. For example, finding the uninitialized variables with the live information may cause false positives.
*   **Troublesome when dealing with indirect access**: If `v` is accessible through another name and initialized through that name, live analysis will not connect the initialization and the use.

## Another Data-flow Problem: Dominance

### Definition
In a CFG with entry block `b0`, node `bi` dominates node `bj` if `bi` lies on every path from `b0` to `bj`.

### Data-flow Equation of Dominance
A block `i` dominates another block `j` if block `i` dominates all of the predecessors of block `j`.

$$DOM(n) = \{n\} \cup \bigcap_{p \in pred(n)} DOM(p)$$

Where:
*   `DOM(n)`: The set of blocks that dominate block `n`.
*   `pred(n)`: The set of predecessors of block `n`.

### Iterative Solver for Dominance
(Similar iterative approach as Liveness, but typically for forward problems, reverse postorder of the CFG itself is efficient.)

## Framework of Solving the Data-flow Problem

We can characterize both of these analyses within a common framework. The data-flow analysis framework has four components:
1.  The direction of analysis (forward or backward)
2.  The values being propagated
3.  Transfer functions for each of the nodes
4.  A meet operator

### Data-flow Values
*   A key component is the set of values that the analysis is computing on.
*   In live variable analysis, the values were sets of variables.
*   In dominance analysis, the values were sets of labels of basic blocks.
*   **Program point**: Points in between the execution of nodes; at the beginning and end of each node.
*   Let `L` be the set of all values that can be assigned to a program point. `l` denotes a single value in `L`.
*   Data-flow values represent some proposition that must hold at the program point they are attached to.

### Transfer Functions
The second part of the data-flow analysis is a set of transfer functions. These functions describe how data-flow values are transformed by a node.
$$f_n: L \rightarrow L$$
$$OUT_{n} = f_n(IN_n)$$ for forward analysis
$$IN_{n} = f_n(OUT_n)$$ for backward analysis

### Meet Operator
The final part of the data-flow analysis which defines how to combine values from multiple incoming edges.
*   For live variable analysis, the meet operator is union ($\cup$).
*   For dominance analysis, the meet operator is intersection ($\cap$).

### Why Derive the Data-flow Analysis Framework?
We derive the data-flow analysis (DFA) framework to ensure the finite descending chain properties by the lattice theory.
*   In a lattice, the meet operator is monotone, meaning the number of values after meet either stays the same or goes only in one direction.
*   The two properties of a monotone transfer function complete DFA:
    *   Closed under composition: composition of monotone transfer function is still monotone transfer function.
    *   Fixed point: `f(x) = x`.

### How to Derive the Data-flow Analysis Framework?
1.  Figure out the data-flow values and their abstract values at the entry and exit of each program point.
2.  Write an equation for every program point relating the values at the entry to that thing at the exit.
3.  Generate equalities according to the edges in the CFG or instruction depending on the program points.
4.  Solve the system of equations.

### Identify the Abstract Values
*   The abstract values are properties we want to know about at program points.
*   The set of abstract values and some binary relation should form a lattice.

### Partial Order
To define lattices, we define the partial order (`S`, `⊑`) first. A partial order is a set `S` of a binary relation `⊑` with the three conditions:
*   **Reflexivity**: $\forall x \in S : x \sqsubseteq x$
*   **Transitivity**: $\forall x, y, z \in S : x \sqsubseteq y \land y \sqsubseteq z \implies x \sqsubseteq z$
*   **Anti-symmetry**: $\forall x, y \in S : x \sqsubseteq y \land y \sqsubseteq x \implies x = y$

### Least Upper Bound (LUB) & Greatest Lower Bound (GLB)
*   `y ∈ S` is an upper bound of `X ⊆ S` if $\forall x \in X : x \sqsubseteq y$.
*   A **least upper bound** `lub(X) ∈ S` is defined as: $X \sqsubseteq lub(X) \land \forall y \in S : X \sqsubseteq y \implies lub(X) \sqsubseteq y$.
*   If `X` has only two elements `{x, y}`, we sometimes use infix notation $x \sqcup y$.
*   Similarly, `glb(X)` is the greatest lower bound of subset `X` and $x \sqcap y$ is the `glb({x, y})`.

### Lattice
*   **Lattice**: A partial order (`S`, `⊑`) where $x \sqcup y$ and $x \sqcap y$ exist for all $x, y \in S$.
*   **Complete lattice**: A lattice where `lub(X)` and `glb(X)` exist for all `X ⊆ S`. Every finite lattice is a complete lattice.

### Motivating Example: Sign Analysis
*   For example, when we want to design an analysis that can find out the possible signs of the integer values of variables and expressions in a given program.
*   We can define the types of the signs into a set `S = {+, -, 0, ⊤, ⊥}`, where `⊤` is values with arbitrary sign, `⊥` is not integer.
*   We can define the binary operation `⊑` as: $x \sqsubseteq y$ means `x` is at least as precise as `y`.
*   This forms a lattice where `⊥` is the least precise (bottom) and `⊤` is the most precise (top).

### Constructing Lattices
To define the lattice of data-flow analysis, we need to map the set of data-flow values to the lattice of abstract values.
We can construct the map lattice `A → L` defined as follows:
`A → L = {[a1→x1, a2→x2, ...] | A={a1,a2,...} ∧ x1,x2,...∈L}`, where `A` is the set of data-flow values, `a→x` means the function that maps `a` to `x`.
$f \sqsubseteq g \iff \forall a \in A : f(a) \sqsubseteq g(a)$ where $f, g \in A \rightarrow L$.

### Equation System over Constraint Functions
(Illustrates how equations are formed based on transfer functions and meet operators across the CFG.)

### Solution of the Equation System
*   A solution to an equation system provides a value from `L` for each variable such that all equations are satisfied.
*   There may be multiple solutions to a set of equations.
*   All solutions to the equations are considered safe, but maybe imprecise.

### Monotone Property on Constraint Functions
*   **Monotone**: A constraint function maps the current map lattice `L1` to the new map lattice `L2`, where `L1` and `L2` map data-flow values to abstract values.
*   A function $f : L_1 \rightarrow L_2$ where `L1` and `L2` are lattices is monotone (or order-preserving) when $\forall x, y \in L_1 : x \sqsubseteq y \implies f(x) \sqsubseteq f(y)$.
*   In program analysis, the intuition of monotonicity is that “more precise input does not result in less precise output.”

### Fixed-point Theorem
*   **Fixed-point**: $x \in L$ is a fixed point for function `f` if $f(x) = x$.
*   **Fixed-point Theorem**: In a complete lattice `L` with finite height, every monotone function $f : L \rightarrow L$ has a unique least fixed point denoted `lfp(f)` defined as:
    *   Observe that the increasing chain: $\bot \sqsubseteq f(\bot) \sqsubseteq f^2(\bot) \sqsubseteq \dots$
    *   Since `L` is assumed to have finite height, we must have a fixed point $f^k(\bot) = f^{k+1}(\bot) = lfp(f)$.
    *   If `x` is another fixed point that $f(x) = x$. Since $\bot \sqsubseteq x$ and `f` is monotone, it follows that $f(\bot) \sqsubseteq f(x) = x$, by induction we get $lfp(f) = f^k(\bot) \sqsubseteq x$.
    *   If `x` is also the `lfp(f)`, similarly, $x \sqsubseteq f^k(\bot)$. By anti-symmetry, $x = f^k(\bot)$ is unique.

### Iterative Algorithm From Fixed-point Theorem
*   Suppose the lattices have finite height and the constraint functions are monotone.
*   The Fixed-point Theorem tells us:
    *   The equation systems over complete lattices always have solutions.
    *   Uniquely most precise solutions always exist.
    *   We can simply compute the increasing chain until the fixed point is reached.

## References
*   Engineering A Compiler (2nd. ed.)
*   Compilers: Principles, Techniques, and Tools (2nd. ed.)
*   Static Program Analysis
*   CS 6120: Advanced Compilers Lessons
*   CS243: Program Analysis and Optimizations