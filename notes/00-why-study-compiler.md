# Why Study Compilers? (Lecture 1)

## Introduction

**I. Why Study Compilers?**
**II. Mathematical Abstractions: with Examples**
**III. Course Syllabus**

*Based on Chapters 1.1-1.5, 8.4, 8.5, 9.1 by M. Lam, CS243: Introduction*

---

## Why Study Compilers?

### Impact!
Techniques in compilers help all programmers.

### Compiler Technology: Key Programming Tool
Compilers bridge the semantic gap between programmers and machines.

*   **Programming languages:**
    *   High-level programming languages
    *   Domain-specific languages
    *   Natural language
*   **Computer architecture:**
    *   RISC vs CISC, Systolic arrays
    *   Locality: Caches, memory hierarchy
    *   Parallelism: Instruction-level parallelism, Multi-processors
*   **Programming Tools:**
    *   Security
    *   Verification
    *   Binary translations

### Compiler Study: a Software Engineering Course

**Trains Good Developers**
*   Reasoning about programs makes better programmers.
*   Tool building: there are programmers and there are tool builders…
*   Excellent software engineering case study: Compilers are hard to build.
    *   Input: all programs
    *   Objectives:
        *   Correctness
        *   Run-time performance (includes minimizing code size)
        *   Compile-time performance
    *   Implementation complexity
*   Methodology for solving complex real-life problems:
    *   Build upon mathematical / programming abstractions.

*Many years of research by many people to solve these problems elegantly.*

### Compilers: Where Theory Meets Practice

*   Desired solutions are often NP-complete / undecidable.
*   **Key to success**: Formulate the right abstraction / approximation.
    *   Can’t be solved by just pure hacking.
        *   Theory aids generality and correctness.
    *   Can’t be solved by just theory.
        *   Experimentation validates & provides feedback to problem formulation.
*   **Tradeoffs**: Generality, power, simplicity, and efficiency.

**Programs:**
*   Static statements
*   Dynamic execution
*   Generated code

**Abstractions:**
*   Graphs
*   Fixed-point solutions
*   Linear Integer programs
*   Linear algebra
*   Logic databases
*   Binary Decision Diagrams (BDD)
*   Neural networks
**Solutions**

### Why Study Compilers? (Recap)

*   **Impact!** Techniques in compilers help all programmers.
*   **Better Programmer**
    *   Reasoning about programs
    *   Mathematical abstractions

### Course Emphasis

*   **Methodology**: Apply the methodology to other real-life problems.
    *   **Design**
        *   Problem statement: Which problem to solve?
        *   New programming abstraction through domain-specific languages.
    *   **Theory and Algorithm**
        *   Theoretical frameworks
        *   Algorithms
    *   **Experimentation**: Hands-on experience (Weekly programming/written homeworks).
*   **Compiler knowledge**:
    *   *Non-goal*: how to build a complete optimizing compiler.
    *   Important algorithms.
    *   Exposure to new ideas.
    *   Background to learn existing techniques.

### Interactive Instruction

*   Compilers are not about memorizing facts – Open-book examinations.
*   **Goal**: Teach how to derive the concepts, so you can apply to new problems.
*   Lectures are interactive. Please come to class.
*   The slides may miss main points to be emphasized in class!
*   These slides supplement lectures. They are not self-contained! They may contain mistakes, corrected in class!

### The Rest of this Lecture (Overview)

*   **Goal**
    *   Overview the course.
    *   Explain why topics were chosen.
    *   Emphasize abstraction methodology.
*   **For each topic**:
    *   Motivate its importance.
    *   Show an example to illustrate the complexity.
    *   Describe the abstraction.
    *   Impact.

---

## 1. Optimizing Compilers for High-Level Programming Languages

*   **Redundancy elimination**
    *   High-level programming languages introduce a lot of redundancies in programs that programmers are not aware of.

*   **Example**: Bubblesort program that sorts array `A` allocated in static storage.

```bubblesort.c#L1-10
for (i = n-2; i >= 0; i--) {
   for (j = 0; j <= i; j++) {
      if (A[j] > A[j+1]) {
         temp = A[j];
         A[j] = A[j+1];
         A[j+1] = temp;
      }
   }
}
```
**Quiz**: What is the best way to speed up this task?

### Code Generated by the Front End (for Bubblesort example)

```frontend_code.txt#L1-35
  i := n-2

S5:  if i<0 goto s1
    j := 0
s4:  if j>i goto s2
    t1 = 4*j
    t2 = &A
    t3 = t2+t1
     t4 = *t3       ;A[j]

    t5 = j+1
    t6 = 4*t5
    t7 = &A
    t8 = t7+t6
    t9 = *t8      ;A[j+1]
    if t4 <= t9 goto s3
    t10 = 4*j
    t11 = &A

  t12 = t11+t10

    temp = *t12   ;temp=A[j]

   t13 = j+1
   t14 = 4*t13
   t15 = &A
   t16 = t15+t14
   t17 = *t16      ;A[j+1]
   t18 = 4*j
   t19 = &A
   t20 = t19+t18  ;&A[j]
  *t20 = t17      ;A[j]=A[j+1]
   t21 = j+1
   t22 = 4*t21
   t23 = &A
   t24 = t23+t22
  *t24 = temp      ;A[j+1]=temp
s3: j = j+1
   goto S4
S2: i = i-1
   goto s5
s1:
```
*(t4=*t3 means read memory at address in t3 and write to t4: *t20=t17: store value of t17 into memory at address in t20)*

### After Optimization (for Bubblesort example)

**Result of applying:**
*   global common subexpression
*   loop invariant code motion
*   induction variable elimination
*   dead-code elimination

to all the scalar and temporary variables.

*These traditional optimizations can make a big difference!*

```optimized_code.txt#L1-26
i = n-2
t27 = 4*i
t28 = &A
t29 = t27+t28
t30 = t28+4

S5: if t29 < t28 goto s1

t25 = t28
t26 = t30

s4: if t25 > t29 goto s2

t4 = *t25    ;A[j]
t9 = *t26    ;A[j+1]
if t4 <= t9 goto s3
temp = *t25  ;temp=A[j]
t17 = *t26   ;A[j+1]
*t25 = t17   ;A[j]=A[j+1]
*t26 = temp  ;A[j+1]=temp
s3: t25 = t25+4
t26 = t26+4
goto S4
S2: t29 = t29-4
goto s5

s1:
```

### DataFlow Framework

*   **High-level programming languages**
    *   Need many optimizations to be efficient.
*   **Data flow**
    *   A general framework.
    *   Finds fixed-point solution to a set of recurrence equations.
    *   Monotonicity.
    *   **Theory**: Prove correctness properties once and for all.
    *   **Implementation**: Same code reused.

### Summary: Data Flow Optimizations

| Topic                 | Abstraction                 | Impact                                                  |
| :-------------------- | :-------------------------- | :------------------------------------------------------ |
| Data flow optimizations (1970-1980s) | Graphs, Recurrent equations, Fixed-point | High-level programming without loss of efficiency.      |

---

## 2. High Performance Computing / Machine Learning

### Large Language Models (LLMs)

*   **GPT-3 in 2020**
    *   Trained to predict the next word.
    *   Unsupervised: 45 TB of Internet text.
    *   175 Billion parameters.
*   **Training**
    *   10,000 V100 GPUs ($4,600,000)
    *   1,287,000 KWh
    *   9 days

### Trends of LLMs

*   Parameters rapidly increasing (e.g., GPT-4 has significantly more than 175 billion).
*   Multimodal: text, images.

### Nvidia Volta GV100 GPU

*   21 Billion transistors
*   815 mm²
*   1455 MHz
*   80 Stream Multiprocessors (SM)

### In Each SM

*   64 FP32 cores
*   64 int cores
*   32 FP64 cores
*   8 Tensor cores

**Tensor Cores**:
*   `D = A x B + C`; `A, B, C, D` are 4x4 matrices.
*   4 x 4 x 4 matrix processing array.
*   1024 floating point ops / clock.

**Performance**:
*   FP32: 15 TFLOPS
*   FP64: 7.5 TFLOPS
*   Tensor: 120 TFLOPS

### Matrix Multiplication

```matrix_multiplication.c#L1-6
for (i = 0; i < n; i++) {
   for (j = 0; j < n; j++) {
      for (k = 0; k < n; k++) {
         Z[i,j] = Z[i,j] + X[i,k]*Y[k,j];
      }
   }
}
```
*   `n³` computation.
*   `n²` threads of parallelism.
*   2 memory operations per multiply-add operation.
*   **Bottleneck**: Memory operations.

### Multiprocessor Architecture

*   Memory accesses are much more expensive than multiply-add.
*   Interconnect becomes a bottleneck – scalability!

### Systolic Arrays

*   Introduced by Kung and Leiserson in 1978.
*   Special-purpose computer architecture for specific algorithms.
*   Processor interconnect matches algorithm communication pattern.
*   Eliminates the memory bottleneck.
*   TPU: 128 x 128 multiply/accumulators in a systolic array.

### Google TPU-v4 Chips, 2022

*   **TPU v4 chip**
    *   Matrix multiplication unit: 128 x 128 multiply/accumulators in a systolic array.
    *   Peak compute per chip: 275 teraflops.
    *   Min/mean/max power: 90/170/192 W.
*   **TPU pod size**
    *   4096 chips.
    *   Peak compute per pod: 1.1 exaflops.

### Google TPU-v4 System

*   Google PaLM: 540B parameters in large language models.
*   Using 6144 TPU v4 chips (1.7 exaflops).

### Principle to Successful Parallelism

*   Parallel execution can be slower than sequential execution due to communication overhead!
*   **Goal**: Maximize parallelism and minimize communication.
*   Principles applicable to uniprocessors (caches) and multiprocessors.

### Blocking for Matrix Multiplication

Blocking is a technique to improve locality and reduce communication overhead.

**Original program:**
```mm_original.c#L1-6
for (i = 0; i < n; i++) {
  for (j = 0; j < n; j++) {
    for (k = 0; k < n; k++) {
      Z[i,j] = Z[i,j] + X[i,k]*Y[k,j];
    }
  }
}
```

**Stripmine 2 outer loops:**
```mm_stripmine.c#L1-10
for (ii = 0; ii < n; ii = ii+B) {
  for (i = ii; i < min(n,ii+B); i++) {
    for (jj = 0; jj < n; jj = jj+B) {
      for (j = jj; j < min(n,jj+B); j++) {
        for (k = 0; k < n; k++) {
          Z[i,j] = Z[i,j] + X[i,k]*Y[k,j];
        }
      }
    }
  }
}
```

**Permute loops (after stripmine):**
```mm_permute.c#L1-10
for (ii = 0; ii < n; ii = ii+B) {
  for (jj = 0; jj < n; jj = jj+B) {
    for (k = 0; k < n; k++) {
      for (i = ii; i < min(n,ii+B); i++) {
        for (j = jj; j < min(n,jj+B); j++) {
          Z[i,j] = Z[i,j] + X[i,k]*Y[k,j];
        }
      }
    }
  }
}
```

### Experimental Results (Blocking)

Blocking can lead to significant speedups (e.g., 20x improvement).

### Affine Framework

*   Many useful loop transformations for locality & parallelism:
    *   Loop interchange, reversal, skewing.
    *   Loop fusion, fission.
    *   Blocking.
*   **Affine Transformations:**
    *   Inspired by systolic arrays.
    *   For dense matrix computations.
    *   A general framework.
    *   Geometric transforms (linear algebra).
    *   Maximizes parallelism and minimizes communication by solving linear inequality constraints.

### Summary: Parallelism and Locality Optimizations

| Topic                 | Abstraction                           | Impact                                                   |
| :-------------------- | :------------------------------------ | :------------------------------------------------------- |
| Parallelism and locality optimizations (1980-1990s) | Integer linear programming, Linear algebra | Hide complexity from programmers, Machine independence code, Systolic arrays. |

---

## 3. Garbage Collection

*   **Automatic memory management**
    *   Hugely improves program robustness and developer productivity.
*   **Original**: Naïve stop-the-world garbage collection.
    *   Stops the program to trace the reachability of all objects.
*   **Key optimizations**: Greatly reduce pause time.
    *   **Incremental**: Break up GC in time.
    *   **Partial**: Break up GC in space.

### Summary: Garbage Collection

| Topic                 | Abstraction                 | Impact                                                     |
| :-------------------- | :-------------------------- | :--------------------------------------------------------- |
| Garbage collection (1990-2000s) | Incremental and partial GC | Remove manual memory management.                            |

---

## 4. Program Analysis for Correctness

### Pointer Alias Analysis Example: SQL Injection Errors

**Scenario**: Hacker, Browser, Web App, Database.
*   Hacker input: "Give me Bob’s credit card #" or "Delete all records".

**SQL Injection Pattern**:
*   User supplies text (`p1 = req.getParameter();`).
*   Text controls the database (`stmt.executeQuery(p2);`).
*   **Pointer analysis**: Can `p1` and `p2` point to the same object?

**In Practice (Example Code Snippets)**:
*   `ParameterParser.java` methods (`getRawParameter`) read user input.
*   `ChallengeScreen.java` builds a SQL query string (`query = tmp.toString();`) using user input (`String user = s.getParser().getRawParameter(USER, "");`).
*   This `query` is then executed (`ResultSet results = statement3.executeQuery(query);`).
*   This chain of events highlights how user-controlled input can flow into sensitive operations, making pointer analysis crucial.

### Automatic Conservative Analysis Generation

With Context-Sensitive & Flow-Insensitive Pointer Analysis.

*   **Programmer**: Security analysis in 10 lines (using PQL).
*   **Compiler Writer**: Pointer analysis in 10 lines (using Datalog).
*   `bddbddb` (BDD-based deductive database) with Active Machine Learning: involves thousands of lines and a year of tuning, based on BDD operations (10,000s-lines library).

### SMT Example: Out-of-Bound Array Access

**Program**:
Assume data array bound is `[0, N-1]`.
```array_access.c#L1-17
1 void ReadBlocks(int data[], int cookie)
2 {
3   int i = 0;
4
5   {
6      int next;
7      next = data[i];
8      if (!(i < next && next < N)) return;
9      i = i + 1;
10     for (; i < next; i = i + 1){
11        if (data[i] == cookie)
12          i = i + 1;
13        else
14          Process(data[i]);
15     }
16   }
17 }
```
**When is the array access in line 7 out of bound?**
`data[i]` in line 7 is accessed when `i=0`. So `data[0]` is accessed. This is in bound if `0 <= 0` and `0 < N`.
The problem arises after `data[i] == cookie`, where `i` is incremented. If `i` becomes `N` or more, subsequent accesses could be out of bounds.

### Satisfiability Modulo Theories (SMT)

*   **Satisfiability**: The problem of determining whether a formula has a model (an assignment that makes the formula true).
*   **SAT**: Satisfiability of propositional formulas.
    *   A model is a truth assignment to Boolean variables.
    *   SAT solvers: Check satisfiability of propositional formulas.
    *   Decidable, NP-complete.
*   **SMT**: Satisfiability modulo theories.
    *   Satisfiability of first-order formulas containing operations from background theories such as arithmetic, arrays, uninterpreted functions, etc.
    *   SMT Solvers: Check satisfiability of SMT formulas with respect to a theory.

### Summary: Program Analysis for Correctness

| Topic                 | Abstraction                                      | Impact                                                 |
| :-------------------- | :----------------------------------------------- | :----------------------------------------------------- |
| Program analysis for correctness (1990-2020s) | Satisfiability modulo theories (SMT), Pointer Alias Analysis | Improve program robustness, Save programmers debugging time. |

---

## 5. NLP: Natural Language Processing

### Large Language Models (LLMs) e.g. GPT-3, ChatGPT

**1. Help programmers with their task:**
*   Writing \"popular\" programs from a description (e.g., website generation).
*   Improving programmers’ productivity with autocompletion (e.g., OpenAI Codex).
    *   Trained to predict the next word on the internet corpus, open-source code.

*This is not a primary focus in this course.*

**2. End user programming (Focus in this course):**
*   Let everybody program with the highest-level programming language: Natural Language.

```nlp_flow.txt#L1-3
Natural
Language
    ↓
Neural
Semantic
Parser
    ↓
Code
```

### Example: A YelpBot

*   Yelp: API access to databases, which include reviews.
*   Created a new DSL: **SUQL** (Structured and Unstructured Query Language).

**SUQL Examples**:
*   `"SELECT *, summary(reviews) FROM restaurants WHERE 'szechuan' = ANY (cuisines) AND location = “Palo Alto” AND answer(reviews, 'does this restaurant serve spicy food?') = 'Yes' LIMIT 1;"`
*   `"SELECT answer(reviews, 'does this restaurant have non-spicy options?') FROM restaurants WHERE name ILIKE ‘Taste' AND location = ‘Palo Alto';"`

### End-User Natural Language Programming

*   New DSLs designed for natural language programming.
*   **Neural semantic parser**: Use LLMs (with fine-tuning).
*   **Examples**: SUQL (Structured and Unstructured Data Queries).
    *   Using LLMs as a subroutine.
    *   Optimizing compiler.

### Summary: End User Programming in Natural Language

| Topic                 | Abstraction               | Impact                                                  |
| :-------------------- | :------------------------ | :------------------------------------------------------ |
| End user programming in natural language (2010-2020s) | Neural semantic parser | New generation of natural, powerful user interfaces. |

---

## Tentative Course Schedule

*(This is a high-level overview, specific topics will be covered in detail)*

| Week | Topic                                   | Details                                                                  |
| :--- | :-------------------------------------- | :----------------------------------------------------------------------- |
| 1    | Course Introduction                     |                                                                          |
| 2    | Data Flow Optimizations                 | Data-flow analysis: introduction, theoretic foundation                   |
| 3    |                                         | Optimization: constant propagation, redundancy elimination                 |
| 4    | Loop Transformations                    |                                                                          |
| 5    |                                         | Register allocation, Instruction scheduling                               |
| 6    | Machine Dependent Optimizations         | Software pipelining, Parallelization                                     |
| 7    |                                         | Loop transformations, Pipelined parallelism, Locality + parallelism     |
| 8    |                                         | Static single assignment & SMT intro                                     |
| 9    | Satisfiability Modulo Theories          | SMT solvers, Advanced techniques                                         |
| 10   | Pointer Analysis                        | Context-sensitive & flow-insensitive analysis                            |
| 11   | Garbage Collection                      |                                                                          |
| 12   | Conversational Interface to Hybrid Data Sources | Natural language → SUQL (Structured & unstructured query language) |
| 13   |                                         |                                                                          |
| 14   |                                         |                                                                          |
| 15   |                                         |                                                                          |
| 16   |                                         |                                                                          |
| 17   |                                         |                                                                          |

---

## Homework

*   Due Wednesday (no need to hand in).
*   Read Chapter 9.1 for introduction of the optimizations.
*   Work out the example on pages 10-12 in this handout.
